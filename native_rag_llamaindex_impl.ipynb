{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMer4iVC/WHEQukbE45sZPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mickymics/RAG-Implementation/blob/main/native_rag_llamaindex_impl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqXy8R3ixr2i"
      },
      "outputs": [],
      "source": [
        "%pip install nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "6mEEEVXIx32S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq jedi"
      ],
      "metadata": {
        "id": "MUCnb08Zx6LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq llama-index"
      ],
      "metadata": {
        "id": "_4DasicWx7Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -Uq llama-index-embeddings-huggingface"
      ],
      "metadata": {
        "id": "FzaBBJVUx8NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Straight forward RAG Steps**"
      ],
      "metadata": {
        "id": "dDOX4s_xyDLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "pyygmYqNyE1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Global Settings vs ServiceContext**\n",
        "\n",
        "**Settings** = “default everywhere” (simple) global style\n",
        "\n",
        "**ServiceContext** = “custom per index” (advanced). - different LLMs or embeddings for different indexes\n",
        "\n",
        "**randomness control knob** [temperature=0.1] - It tells the model how creative or deterministic it should be when generating text. Consistent and fact-focused, good for retrieval (RAG)\n",
        "\n",
        "**Low temperature** (0.0 – 0.2) - very predictable, safe, repetitive answers."
      ],
      "metadata": {
        "id": "rV8cdA8U0z-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings, VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.llms.openai import OpenAI\n",
        "# from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core import ServiceContext\n",
        "\n",
        "\n",
        "# Document objects\n",
        "documents = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
        "len(documents)\n",
        "\n",
        "# Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
        "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0.1)"
      ],
      "metadata": {
        "id": "A1wR-LdzyGsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Default Behavior**\n",
        "\n",
        "When executing *VectorStoreIndex.from_documents()* line, it does the following internally:\n",
        "\n",
        "Before embedding, chunking happens implicitly; **LlamaIndex** automatically splits the documents into smaller pieces a.k.a chunks (default ~512 tokens).\n",
        "\n",
        "Chunks are then embedded and stored into the multi dimentional vector index.\n",
        "\n",
        "**By default, LlamaIndex uses SentenceSplitter with chunk_size=512 tokens** and some overlap and it defaults to **OpenAI Embeddings (text-embedding-ada-002)** and **OpenAI LLM (gpt-3.5-turbo)**"
      ],
      "metadata": {
        "id": "8e7_D-ib1cPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "sUSzBvAT8Guq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stages of querying:**\n",
        "\n",
        "**Retrieval**: retrieves the most relevant chunks from the vector store (default similarity_top_k=2).\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "index.as_query_engine()* -> **returns a Retriever**\n",
        "\n",
        "# Source Node 1/2 and Source Node 2/2 → meaning two chunks (nodes) were retrieved as the top candidates.\n",
        "\n",
        "```\n",
        "**Postprocessing**: Nodes retrieved are optionally reranked, transformed, or filtered based on the metadata or keywords attached.\n",
        "\n",
        "**Response synthesis**: Both of these retrieved chunks are then sent as context to the LLM along with the query/prompt to get the final response and sources nodes (if mentioned) with pprint_response(response, show_source=True)"
      ],
      "metadata": {
        "id": "yp1pvB8y8V29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval Technique**\n",
        "\n",
        "User query also embedded and vectorized the same way and stored into the multi dimentional vector index.\n",
        "\n",
        "When query, LlamaIndex retrieves the most relevant or nearest neighbours chunks from the vector store using cosine similarity.\n",
        "\n",
        "**By default, LlamaIndex uses OpenAI's gpt-3.5-turbo model**"
      ],
      "metadata": {
        "id": "QIxPhon382oP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.pprint_utils import pprint_response\n",
        "\n",
        "response = query_engine.query(\"What is Prometheus namespace\")\n",
        "pprint_response(response, show_source=True)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Fe6d_zQ-8MZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}